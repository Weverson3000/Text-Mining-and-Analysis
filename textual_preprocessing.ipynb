{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgatwhTEw4Sh0td8SCk2PF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Weverson3000/Text-Mining-and-Analysis/blob/main/textual_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AS01: Pré-processamento Textual\n",
        "\n",
        "\n",
        "*  Curso: Engenharia da Computação\n",
        "*  Materia: Tópicos em Computação III\n",
        "*  Aluno: Weverson Euzebio Forbes Silva"
      ],
      "metadata": {
        "id": "nWUPyphIgHe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregamento do arquivo Shakespeare.txt de um link publico\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8gt9gpqQAkej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "# URL compartilhada do Google Drive\n",
        "url = \"https://drive.google.com/uc?id=12e48l57FEPwaKXpo_aeB2tBy4kan4U8V\"\n",
        "\n",
        "# Caminho para salvar o arquivo localmente\n",
        "output = \"arquivo_baixado.txt\"\n",
        "\n",
        "# Baixando o arquivo\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Lendo o conteúdo do arquivo\n",
        "with open(output, 'r') as file:\n",
        "    conteudo = file.read()\n",
        "\n",
        "#print(conteudo)\n"
      ],
      "metadata": {
        "id": "VimP2XMlLwAP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0cc83db-96b7-48d2-f4cf-c27600efe90e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12e48l57FEPwaKXpo_aeB2tBy4kan4U8V\n",
            "To: /content/arquivo_baixado.txt\n",
            "100%|██████████| 100k/100k [00:00<00:00, 58.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Normalização"
      ],
      "metadata": {
        "id": "7RKvR3cUDzeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# Função que remove acentos e diacríticos\n",
        "def remover_acentos(texto):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', texto) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "# Função que normalização o texto\n",
        "def normalizar_texto(texto):\n",
        "    texto = texto.lower()  # Lower case\n",
        "    texto = remover_acentos(texto)  # Remover acentos e diacríticos\n",
        "    texto = re.sub(r'\\b[a-z]{1,2}\\.\\b', '', texto)  # Canonicalizar acrônimos (ex: Ms. -> Ms)\n",
        "    texto = re.sub(r'\\$', 'dollar', texto)  # Canonicalizar moeda\n",
        "    texto = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{4}', 'data', texto)  # Canonicalizar data\n",
        "    texto = re.sub(r'-', ' ', texto)  # Canonicalizar palavras com hífen\n",
        "    texto = re.sub(r'[^\\w\\s]', '', texto)  # Remover pontuação\n",
        "    texto = re.sub(r'[^A-Za-z0-9\\s]', '', texto)  # Remover caracteres especiais\n",
        "    return texto\n",
        "\n",
        "# Texto de entrada para teste\n",
        "#texto = \"\"\"It's true, Ms. Martha Töpfer! $3.00 on 3/21/2023 in cash for an ice-cream in the U.S. market? :-( #Truth\"\"\"\n",
        "\n",
        "texto = conteudo\n",
        "\n",
        "# Aplicando a normalização\n",
        "texto_normalizado = normalizar_texto(texto)\n",
        "\n",
        "# Salvar o arquivo normalizado\n",
        "with open('Shakespeare_Normalized.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(texto_normalizado)\n"
      ],
      "metadata": {
        "id": "j56y9LAvN_wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenização"
      ],
      "metadata": {
        "id": "Qzzf8tm4OGZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, TreebankWordTokenizer, WordPunctTokenizer, TweetTokenizer, MWETokenizer\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "from gensim.utils import tokenize as gensim_tokenize\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "# Carregar o texto normalizado\n",
        "with open('Shakespeare_Normalized.txt', 'r', encoding='utf-8') as file:\n",
        "    texto_normalizado = file.read()\n",
        "\n",
        "# Definir algumas expressões pre determinadas (MWE) para o MWETokenizer\n",
        "mwe_tokenizer = MWETokenizer([('New', 'York'), ('San', 'Francisco'), ('et', 'al')], separator='_')\n",
        "\n",
        "# Tokenizações\n",
        "tokenizations = {\n",
        "    \"Whitespace\": texto_normalizado.split(),\n",
        "    \"NLTK Word\": word_tokenize(texto_normalizado),\n",
        "    \"NLTK Treebank\": TreebankWordTokenizer().tokenize(texto_normalizado),\n",
        "    \"NLTK Word Punctuation\": WordPunctTokenizer().tokenize(texto_normalizado),\n",
        "    \"NLTK Tweet\": TweetTokenizer().tokenize(texto_normalizado),\n",
        "    \"NLTK MWE\": mwe_tokenizer.tokenize(word_tokenize(texto_normalizado)),\n",
        "    \"TextBlob\": TextBlob(texto_normalizado).words,\n",
        "    \"spaCy\": [token.text for token in spacy.load('en_core_web_sm')(texto_normalizado)],\n",
        "    \"Gensim\": list(gensim_tokenize(texto_normalizado)),\n",
        "    \"Keras\": text_to_word_sequence(texto_normalizado)\n",
        "}\n",
        "\n",
        "# Salvar as tokenizações\n",
        "for i, (name, tokens) in enumerate(tokenizations.items(), 1):\n",
        "    with open(f'Shakespeare_Normalized_Tokenized{i:02}.txt', 'w', encoding='utf-8') as file:\n",
        "        file.write(' '.join(tokens))\n"
      ],
      "metadata": {
        "id": "Uq8efGk_OJVj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d52ac5-4b44-430b-e9ad-2ef4b2d787cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop-Words Removal"
      ],
      "metadata": {
        "id": "ZzPAACpJOMX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Baixar os stop-words\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Carregar os stop-words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Carregar a tokenização desejada usando so o NLTK Word Tokenizer, indicado no enunciado da atividade\n",
        "with open('Shakespeare_Normalized_Tokenized02.txt', 'r', encoding='utf-8') as file:\n",
        "    tokens = file.read().split()\n",
        "\n",
        "# Remover stop-words\n",
        "tokens_sem_stopwords = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "# Salvar o arquivo sem stop-words\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(' '.join(tokens_sem_stopwords))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXTiXUJvOWgM",
        "outputId": "d617c6bc-0c33-48ce-d0aa-d291768d2464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Lemmatization"
      ],
      "metadata": {
        "id": "OKZ8bJYHOatQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Baixar o recurso wordnet\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Inicializar o lematizador\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Carregar o texto sem stop-words\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord.txt', 'r', encoding='utf-8') as file:\n",
        "    tokens_sem_stopwords = file.read().split()\n",
        "\n",
        "# Lematizar o texto\n",
        "tokens_lematizados = [lemmatizer.lemmatize(token) for token in tokens_sem_stopwords]\n",
        "\n",
        "# Salvar o arquivo lematizado\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(' '.join(tokens_lematizados))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9M_uGsgOaM9",
        "outputId": "e41d1af7-119e-4fc8-9ee5-9d8c9ef2ac05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "# Inicializar os stemmers\n",
        "porter_stemmer = PorterStemmer()\n",
        "snowball_stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Carregar o texto lematizado\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt', 'r', encoding='utf-8') as file:\n",
        "    tokens_lematizados = file.read().split()\n",
        "\n",
        "# Aplicar Porter Stemmer\n",
        "tokens_porter = [porter_stemmer.stem(token) for token in tokens_lematizados]\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming01.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(' '.join(tokens_porter))\n",
        "\n",
        "# Aplicar Snowball Stemmer\n",
        "tokens_snowball = [snowball_stemmer.stem(token) for token in tokens_lematizados]\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming02.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(' '.join(tokens_snowball))\n"
      ],
      "metadata": {
        "id": "rvwQgj29Opvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análise do Vocabulário"
      ],
      "metadata": {
        "id": "TJJufhYfOs8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "# Função que salvar a análise do vocabulário\n",
        "def salvar_analise_vocabulario(tokens, nome_arquivo):\n",
        "    vocabulario = Counter(tokens)\n",
        "    with open(nome_arquivo, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Token\", \"Ocorrências\", \"Tamanho\"])\n",
        "        #print(chegou ate 1)\n",
        "        for token, count in vocabulario.items():\n",
        "            writer.writerow([token, count, len(token)])\n",
        "            #print(chegou ate 2)\n",
        "\n",
        "\n",
        "# Carregar e analisar vocabulários\n",
        "tokens_lematizados = open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt', 'r', encoding='utf-8').read().split()\n",
        "tokens_porter = open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming01.txt', 'r', encoding='utf-8').read().split()\n",
        "tokens_snowball = open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming02.txt', 'r', encoding='utf-8').read().split()\n",
        "\n",
        "# Salvar os CSVs\n",
        "salvar_analise_vocabulario(tokens_lematizados, 'Shakespeare_Vocabulary_Lemmatized.csv')\n",
        "salvar_analise_vocabulario(tokens_porter, 'Shakespeare_Vocabulary_Porter.csv')\n",
        "salvar_analise_vocabulario(tokens_snowball, 'Shakespeare_Vocabulary_Snowball.csv')\n",
        "#print(chegou ate 2)\n",
        "\n",
        "# Análise comparativa\n",
        "def analisar_vocabulario(vocabulario):\n",
        "    num_tokens = len(vocabulario)\n",
        "    ocorrencias = sum(vocabulario.values())\n",
        "    tamanho_medio = sum(len(token) for token in vocabulario) / num_tokens\n",
        "    #print(chegou ate 3)\n",
        "    return num_tokens, ocorrencias / num_tokens, tamanho_medio\n",
        "\n",
        "analise_lematizada = analisar_vocabulario(Counter(tokens_lematizados))\n",
        "analise_porter = analisar_vocabulario(Counter(tokens_porter))\n",
        "analise_snowball = analisar_vocabulario(Counter(tokens_snowball))\n",
        "\n",
        "\n",
        "# Salvar análise comparativa\n",
        "with open('Shakespeare_Vocabulary_Analysis.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(\"Lemmatizer:\\n\")\n",
        "    file.write(f\"Vocabulário: {analise_lematizada[0]}\\n\")\n",
        "    file.write(f\"Média de ocorrências: {analise_lematizada[1]:.2f}\\n\")\n",
        "    file.write(f\"Tamanho médio dos tokens: {analise_lematizada[2]:.2f}\\n\\n\")\n",
        "    #print(chegou ate 4)\n",
        "\n",
        "    file.write(\"Porter Stemmer:\\n\")\n",
        "    file.write(f\"Vocabulário: {analise_porter[0]}\\n\")\n",
        "    file.write(f\"Média de ocorrências: {analise_porter[1]:.2f}\\n\")\n",
        "    file.write(f\"Tamanho médio dos tokens: {analise_porter[2]:.2f}\\n\\n\")\n",
        "\n",
        "    file.write(\"Snowball Stemmer:\\n\")\n",
        "    file.write(f\"Vocabulário: {analise_snowball[0]}\\n\")\n",
        "    file.write(f\"Média de ocorrências: {analise_snowball[1]:.2f}\\n\")\n",
        "    file.write(f\"Tamanho médio dos tokens: {analise_snowball[2]:.2f}\\n\")\n",
        "    #print(chegou ate 5)"
      ],
      "metadata": {
        "id": "TJxcld08Othm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## \"Os arquivos gerados estaram localizados no seguinte diretório: /content/sample_data.\""
      ],
      "metadata": {
        "id": "a1f2qnUSlHfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referências bibliográficas:\n",
        "\n",
        "Manning, Raghavan & Schütze, 2008\n",
        "\n",
        "Slides e Videos:\n",
        "LC02: Google Colaboratory,\n",
        "LC04: Normalização de Texto,\n",
        "LC05: Tokenização,\n",
        "LC06: Stop-Words,\n",
        "LC07: Lematização,\n",
        "LC08: Stemming\n",
        "\n",
        "**Biblioteca `gdown`:** A biblioteca `gdown` para download de arquivos do Google Drive. **Referência**: https://pypi.org/project/gdown/\n",
        "\n",
        "**Documentação Python (para funçao `open()`):** A função `open()` Permite abrir e ler arquivos de texto em python. **Referência**: https://docs.python.org/3/library/functions.html#open\n",
        "\n",
        "Documentação biblioteca nltk para tokenização, remoção de stopwords, lematização e stemming:\n",
        "\n",
        "Documentação Bird, S., Klein, E., & Loper, E. (2009). Natural Language Processing with Python. O'Reilly Media, Inc.\n",
        "Biblioteca spacy para tokenização:\n",
        "\n",
        "\n",
        "Documentação. https://keras.io.\n",
        "Uso do TextBlob para tokenização e processamento de texto:\n",
        "\n",
        "Documentação https://textblob.readthedocs.io. Biblioteca unicodedata para remoção de acentos e diacríticos:\n",
        "\n",
        "Tokenização usando nltk:\n",
        "\n",
        "Como tokenizar uma frase em NLTK?\n",
        "https://stackoverflow.com/questions/15547409/\n",
        "Remoção de stopwords com nltk:\n",
        "\n",
        "Removing stop words from a string in Python\n",
        "https://stackoverflow.com/questions/5486337/\n",
        "Lematização usando WordNetLemmatizer:\n",
        "\n",
        "Qual é a diferença entre stemming e lemmatization?\n",
        "https://stackoverflow.com/questions/437489\n",
        "Uso do PorterStemmer para stemming:\n",
        "\n",
        "Porter stemmer in NLTK\n",
        "https://stackoverflow.com/questions/36182502/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sRoSpWI6BQoe"
      }
    }
  ]
}